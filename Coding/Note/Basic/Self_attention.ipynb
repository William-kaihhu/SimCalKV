{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0d0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttentionV1(nn.Module):\n",
    "    def __init__(self, hidden_dim: int = 728) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.q_proj = nn.Linear(hidden_dim, hidden_dim) # (in_feature, out_feature)\n",
    "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
    "     # the Shape of X is (batch_size, seq_length, hidden_dim)\n",
    "    def forward(self, X):\n",
    "        Q = self.q_proj(X)\n",
    "        K = self.k_proj(X)\n",
    "        V = self.v_proj(X)\n",
    "\n",
    "        attention_value = torch.matmul(Q, K.transpose(-1, -2))\n",
    "\n",
    "        attention_weight = torch.softmax(\n",
    "            attention_value / math.sqrt(self.hidden_dim), \n",
    "            dim=-1\n",
    "        )\n",
    "        print(attention_weight) # the Shape is (batch_size, seq_length, seq_length)\n",
    "\n",
    "        output = torch.matmul(attention_weight, V)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2732937",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(3, 2, 4)\n",
    "net = SelfAttentionV1(4)\n",
    "print(X)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e4589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SelfAttentionV2(nn.Module):\n",
    "    def __init__(self, dim, dropout_rate = 0.1) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.proj = nn.Linear(dim, dim * 3)\n",
    "        self.attention_dropout = nn.Dropout(dropout_rate)\n",
    "        self.output_proj = nn.Linear(dim, dim) # 可选\n",
    "\n",
    "    def forward(self, X, attention_mask = None):\n",
    "        QKV = self.proj(X)\n",
    "        Q, K, V = torch.split(QKV, self.dim, dim=-1)\n",
    "        attention_score = Q @ K.transpose(-1, -2) / math.sqrt(self.dim)\n",
    "        if attention_mask is not None: \n",
    "            # 如果有attention_mask，就对无效位置（即attention_mask=0的位置）填充一个极小值\n",
    "            # 这样softmax时就会让这个无效位置权重接近0，即“忽略”\n",
    "            attention_score = attention_score.masked_fill(\n",
    "                attention_mask == 0,\n",
    "                float(\"-1e20\")\n",
    "            )\n",
    "        print(f\"score before softmax:\\n {attention_score}\")\n",
    "        attention_weight = torch.softmax(\n",
    "            attention_score,\n",
    "            dim=-1\n",
    "        )\n",
    "        print(f\"weight after softmax:\\n {attention_weight}\")\n",
    "        attention_weight = self.attention_dropout(attention_weight)\n",
    "        print(f\"weight after softmax & dropout:\\n {attention_weight}\")\n",
    "        attention_result = attention_score @ V\n",
    "        print(f\"attention result(before linear layer):\\n {attention_result}\")\n",
    "        output = self.output_proj(attention_result)\n",
    "\n",
    "        return output # 可以看到每个token的向量表示，形状是（batch_size, seq_length, dim)\n",
    "    \n",
    "X = torch.rand(3, 4, 2)\n",
    "mask = torch.tensor([\n",
    "    [1, 1, 1, 0],\n",
    "    [1, 1, 0, 0],\n",
    "    [1, 0, 0, 0]\n",
    "])\n",
    "print(f\"original mask shape:{mask.shape}\")\n",
    "mask = mask.unsqueeze(dim=1).repeat(1, 4, 1) \n",
    "# unsqueeze(dim=1)是在第1维（即第二个维度）上增加一个维度，repeat(1, 4, 1)重复4次，4代表seq_length\n",
    "# 让mask变成和attention_weight一样的形状，即（batch_size, seq_length, seq_length）\n",
    "print(f\"repeat mask shape:{mask.shape}\")\n",
    "net = SelfAttentionV2(2)\n",
    "net(X, mask) # 经过一个transformer block之后，shape不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a5f672b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测的下一个 token id: 872\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttentionV3(nn.Module):\n",
    "    def __init__(self, dim: int, dropout_rate: float = 0.1) -> None:\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.q = nn.Linear(dim, dim)\n",
    "        self.k = nn.Linear(dim, dim)\n",
    "        self.v = nn.Linear(dim, dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, X, attention_mask=None):\n",
    "        Q = self.q(X)\n",
    "        K = self.k(X)\n",
    "        V = self.v(X)\n",
    "\n",
    "        scores = Q @ K.transpose(-1, -2) / math.sqrt(self.dim)\n",
    "        if attention_mask is not None:\n",
    "            scores = scores.masked_fill(attention_mask == 0, float('-inf'))\n",
    "        attention_weights = torch.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "        output = attention_weights @ V\n",
    "        return output\n",
    "\n",
    "class SimpleTransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, vocab_size, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = SelfAttentionV3(dim, dropout_rate)\n",
    "        self.ln1 = nn.LayerNorm(dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, dim * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim * 4, dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(dim)\n",
    "        self.output_layer = nn.Linear(dim, vocab_size)\n",
    "\n",
    "    def forward(self, X, attention_mask=None):\n",
    "        # Self-Attention + Residual + LayerNorm\n",
    "        attn_out = self.attention(X, attention_mask)\n",
    "        x = self.ln1(X + attn_out)\n",
    "        # Feed Forward + Residual + LayerNorm\n",
    "        ffn_out = self.ffn(x)\n",
    "        x = self.ln2(x + ffn_out)\n",
    "        # 输出映射到词表\n",
    "        logits = self.output_layer(x)  # shape: [batch, seq_len, vocab_size]\n",
    "        return logits\n",
    "\n",
    "# 测试代码\n",
    "batch_size = 1\n",
    "seq_len = 4\n",
    "hidden_dim = 8\n",
    "vocab_size = 1000\n",
    "\n",
    "# 假输入（随机初始化）\n",
    "X = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "\n",
    "# 全部token可见mask\n",
    "attention_mask = torch.ones(batch_size, seq_len, seq_len)\n",
    "\n",
    "model = SimpleTransformerBlock(hidden_dim, vocab_size)\n",
    "\n",
    "logits = model(X, attention_mask)\n",
    "\n",
    "# 取最后一个token的logits预测下一个词\n",
    "last_token_logits = logits[:, -1, :]\n",
    "probs = torch.softmax(last_token_logits, dim=-1)\n",
    "predicted_token_id = torch.argmax(probs, dim=-1)\n",
    "\n",
    "print(\"预测的下一个 token id:\", predicted_token_id.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
