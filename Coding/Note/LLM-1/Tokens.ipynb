{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d20c4a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用MPS设备\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# 检测可用的设备\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"使用MPS设备\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"使用CUDA设备\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"使用CPU设备\")\n",
    "\n",
    "model_path = \"/Applications/All/py_code/pythonProject/LLM-Inference-self/TinyLLama\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model = model.to(device)  # 将模型移动到指定设备\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f31816d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 14240,   278,  1494, 10541,   964, 10013, 29901,   525, 10994,\n",
       "         29892,   920,   526,   366, 17901, 29966, 29989,   326, 29918,  2962,\n",
       "         29989, 29958,   465, 22137]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
       "       device='mps:0')}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"translate the following sentence into Chinese: 'Hello, how are you?'<|im_start|>assistant\"\n",
    "\n",
    "# 对输入进行编码\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ece93cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成的文本：\n",
      "<s> translate the following sentence into Chinese: 'Hello, how are you?'<|im_start|>assistant: \"Hello, how are you?\"</s>\n"
     ]
    }
   ],
   "source": [
    "generation_output = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    max_new_tokens=100\n",
    ")\n",
    "# 这里是生成输出的解码和打印部分\n",
    "generated_text = tokenizer.decode(generation_output[0], )\n",
    "print(\"生成的文本：\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d21cc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用MPS设备\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "# 检测可用的设备\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"使用MPS设备\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"使用CUDA设备\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"使用CPU设备\")\n",
    "\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64886896",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=50,\n",
    "    do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a718c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"介绍一下李白<|im_start|>assistant\"\n",
    "output = generator(prompt)\n",
    "print(output[0][\"generated_text\"]) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caee3d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "print(text)\n",
    "generator(text)[0][\"generated_text\"] # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4278206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = \"The capital of France is\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
